View(mod_margins)
mod_margins
mod
mar_cl <- margins::margins(mod[[1]])
mar_cl
mar_cl <- margins::marginal_effects(mod[[1]])
mar_cl
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginals(mod[[i]])
}
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::margins(mod[[i]])
}
View(mod_margins)
mod_margins <- list()
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
View(mod_margins)
library(mitools)
library(miceadds)
data(data.ma05)
dat <- data.ma05
summary(dat)
resp <- dat[, - c(1:2) ]
library(mitools)
library(miceadds)
data(data.ma05)
dat <- data.ma05
summary(dat)
library(estimatr)
mod <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
mod[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
resp <- dat[, - c(1:2) ]
imp <- mice::mice( resp, method="norm", maxit=3, m=6 )
datlist <- miceadds::mids2datlist( imp )
mod <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
mod[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
mar_mod <- lapply(Mod, FUN=function(rr){
with(rr, margins::margins(lm,vcov= vars))
})
library(margins)
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
# For single model in `mod`:
mar_cl <- margins::margins(mod[[1]])
mar_cl
summary(mod)
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
mod_margins <- list()
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
View(mod_margins)
summary(mod_margins)
mod_margins
mod
mod[[1]]$p.value
mod[[]]$p.value
mod_pvs_se <- data.frame()
mod[[1]]$std.error
for(i in seq_along(mod)){
mod_pvs_se$p-value <- mod[[i]]$p.value
mod_pvs_se$SE <- mod[[i]]$std.error
}
for(i in seq_along(mod)){
mod_pvs_se$p_value <- mod[[i]]$p.value
mod_pvs_se$SE <- mod[[i]]$std.error
}
require(data.table)
set.seed(1)
DT1 <- data.table(loc = c("L1","L2"), product = c("P1","P2","P3"), qty = runif(12))
DT2 <- data.table(product = c("P1","P2","P3"), family = c("A","A","B"), price = c(5,7,10))
require(data.table)
set.seed(1)
DT1 <- data.table(loc = c("L1","L2"), product = c("P1","P2","P3"), qty = runif(12))
DT2 <- data.table(product = c("P1","P2","P3"), family = c("A","A","B"), price = c(5,7,10))
View(DT1)
View(DT2)
library(readxl)
tech_merged_channels <- read_excel("merged_video_stats.xlsx")
setwd("~/code/R/DataAccess-Extraction")
library(readxl)
tech_merged_channels <- read_excel("merged_video_stats.xlsx")
library(broom)
library(dplyr)
library(tibble)
library(tuber)
library(naniar)
library(ggplot2)
library(magrittr)
library(reshape2)
library(corrplot)
library(ggthemes)
library(tidyverse)
library(PerformanceAnalytics)
i <- c(7, 8, 9, 10, 11, 13, 14, 15, 17, 18, 20, 21, 22, 24)
tech_merged_channels[, i] <- apply(tech_merged_channels[ , i], 2,
function(x) as.numeric(as.character(x)))
# Change the date to date:
tech_merged_channels$PublishedDate <- as.Date(tech_merged_channels$PublishedDate)
# Factor:
tech_merged_channels$channel_title <- as.factor(tech_merged_channels$channel_title)
# Check for any missing values:
summary(tech_merged_channels)
# To visualise this:
gg_miss_var(tech_merged_channels)
# Trends in missing values
gg_miss_upset(tech_merged_channels)
# Heatmap
vis_miss(tech_merged_channels)
# 0.3%
# Number of columns with missing values.
n_var_miss(tech_merged_channels)
# [1] 9
# Number of rows with missing values
sum(apply(tech_merged_channels, 1, anyNA))
# [1] 77
#Count of missing values per column:
data.frame(sapply(tech_merged_channels, function(y) sum(length(which(is.na(y))))))
# Missing data by Year
gg_miss_fct(x = tech_merged_channels, fct = PublishedYear) +
labs(title = "Year NA in YouTube Data")
# Missing data by Channel:
gg_miss_fct(x = tech_merged_channels, fct = channel_title) +
labs(title = "Channel NA in YouTube Data ")
# Remove missing values.
tech_merged_channels <- tech_merged_channels[complete.cases(tech_merged_channels), ]
# You can confirm this by running the line 249
# Visualization:
# Significant Outliers in Views:
library(ggpubr)
library(rstatix)
# Data Exploration:
# View the distribution of views:
tech_merged_channels %>%
ggplot( aes(x=viewCount)) +
geom_histogram(bins = 50, fill="#0c4c8a", color="#e9ecef", alpha=0.9) +
theme_minimal() +
labs(title = "YouTube View",
subtitle = "YouTube Views from The marquesbrownlee, Dave2d, UltraLinx",
x = "View Count",
y = " ",
caption = "YouTube")
# To see outliers clearly we shall have a boxplot:
tech_merged_channels %>%
ggplot(aes(x="", y = viewCount)) +
geom_boxplot(outlier.colour="red")
theme_minimal()
# By Channel title:
tech_merged_channels %>%
ggplot(aes(x=viewCount, y = channel_title)) +
geom_boxplot(outlier.colour="red") +
theme_calc()+ scale_colour_calc() +
ggtitle("YouTube Data") +
labs(title = "YouTube View Outliers",
subtitle = "YouTube Views from The marquesbrownlee, Dave2d, UltraLinx",
x = "View Count",
y = "Channel",
caption = " ©YouTube")
# To check outliers:
Outliers <-  tech_merged_channels %>%
group_by(channel_title) %>%
identify_outliers(viewCount)
# Significant outliers i.e. is.extreme = TRUE
Significant_Outliers <- Outliers[which(Outliers$is.extreme == TRUE),]
#Plotting the data with and without the Outliers comparing them with like counts
par(mfrow=c(1, 2))
plot(tech_merged_channels$viewCount, tech_merged_channels$likeCount,
main="With Outliers",
xlab = "View Count",
ylab = "Like Count",
pch="*",
col="red", cex=2)
abline(lm(likeCount ~ viewCount, data=tech_merged_channels), col="blue", lwd=3, lty=2)
# Without
Without_Outliers_data <- tech_merged_channels[-which(tech_merged_channels$viewCount %in% c(Outliers$viewCount)),]
plot(Without_Outliers_data$viewCount, Without_Outliers_data$likeCount,
main="Without Outliers",
xlab = "View Count",
ylab = "Like Count",
pch="*",
col="red", cex=2)
abline(lm(likeCount ~ viewCount, data=Without_Outliers_data), col="blue", lwd=3, lty=2)
data <- Without_Outliers_data[,c("viewCount","likeCount","dislikeCount",
"commentCount", "subscriberCount", "channelVideoCount",
"channelViewCount", "PrevCommentCount", "PrevDislikeCount",
"PrevLikeCount", "PrevViewCount")]
date_data <- data.frame(
# Find the number days in between posting videos:
Days = as.numeric(difftime(strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$publication_date), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"),strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$PrevPublishedAt), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"), units = "days")),
# Find the number of hours in between posting videos:
Hours_published = as.numeric(difftime(strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$publication_date), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"),strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$PrevPublishedAt), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"), units = "hours")),
# Age of Channel to Video publication:
Age_Channel = Without_Outliers_data$PublishedYear - Without_Outliers_data$ChannelAge)
data <- add_column(data, date_data, .after = 11)
# Correlation heatmap
cormat <- round(cor(data), 2)
get_upper_tri <- function(cormat){
cormat[lower.tri(cormat)]<- NA
return(cormat)
}
upper_tri <- get_upper_tri(cormat)
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+ # minimal theme
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
ggheatmap +
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
axis.title.x = element_blank(),
axis.title.y = element_blank(),
panel.grid.major = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.ticks = element_blank(),
legend.justification = c(1, 0),
legend.position = c(0.6, 0.7),
legend.direction = "horizontal")+
guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
title.position = "top", title.hjust = 0.5))
# For Correlation significance:
PerformanceAnalytics::chart.Correlation(data, pch=19)
# The view counts by day:
tech_merged_channels %>%
ggplot(aes(x=PublishedDate,y=viewCount)) +
geom_line(color="steelblue", size = 0.5) +
scale_color_manual(values = c("#00AFBB", "#E7B800","#FF0000")) +
labs(title = "YouTube Videos Posted Trend from 2008 - 2021",
caption = " Source: YouTube") +
xlab("Date") + ylab("View Count") +
theme_minimal()
# There has been an increase in views with the age of the account.
# By individual accounts:
tech_merged_channels %>% ggplot(aes(x = PublishedDate , y = viewCount, color = channel_title)) +
geom_line() +
scale_color_manual(values = c("#00AFBB", "#E7B800","#FF0000")) +
labs(title = "YouTube Videos Posted Trend from 2008 - 2021",
subtitle = "Trends by Channels",
caption = " Source: YouTube") +
xlab("Date") + ylab("View Count") +
theme_minimal()
# Time Visualization:
library(lubridate)
time <- data.frame( Date = gsub("T", " ", tech_merged_channels$publication_date))
time <- data.frame(Date = as.POSIXct(gsub("Z", "", time$Date), format = "%Y-%m-%d %H:%M:%S"))
# Time of Day
time <- time %>%
add_column(Time = format(time$Date, format = "%H:%M:%S"),
# Time of Day
TOD = with(time, ifelse(Time > "053000" & Time < "120000", "Morning",
ifelse(Time >= "120000" & Time < "170000", "Afternoon",
ifelse(Time >= "170000" & Time < "200000", "Evening", "Night")))),
# Day of Week
DOW = weekdays(time$Date, abbreviate = F),
year = tech_merged_channels$PublishedYear,
channel_title = tech_merged_channels$channel_title
)
# Count by Day of the week & Time of Day:
time$DOW <- factor(time$DOW, levels= c("Sunday", "Monday",
"Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
time %>%
ggplot(aes(x=DOW, fill = TOD)) + geom_bar(position = "stack") +
coord_flip() +
scale_fill_brewer( palette = "YlGnBu" ) +
theme_minimal() + theme( legend.position = "bottom" ) +
labs(title = "YouTube Daily Trend",
subtitle = "Daily Trends by Time of Week",
caption = " Source: ©YouTube") +
xlab("") + ylab("View Count")
# By DOW by Channel:
time %>%
ggplot(aes(x=DOW, fill = channel_title)) + geom_bar(position = "stack") +
coord_flip() +
scale_fill_brewer( palette = "YlGnBu" ) +
theme_minimal() + theme( legend.position = "bottom" ) +
labs(title = "YouTube Daily Trend",
subtitle = "Daily Trends by Channel",
caption = " Source: ©YouTube") +
xlab("") + ylab("View Count")
library(randomForest)
library(rsample) #***
library(ranger) #***
library(caret) #***
library(h2o)
set.seed(123)
youtube_split <- createDataPartition(data$viewCount, p = .7,
list = FALSE,
times = 1)
youtube_train <- data[youtube_split,]
youtube_test <- data[-youtube_split,]
rforest <- train(viewCount ~ ., data = youtube_train,
method = "rf",
ntree = 500,
trControl = trainControl(method = "none"),
tuneGrid = data.frame(mtry = 4))
rforest
set.seed(123)
youtube_split <- createDataPartition(data$viewCount, p = .7,
list = FALSE,
times = 1)
youtube_train <- data[youtube_split,]
youtube_test <- data[-youtube_split,]
# Default RF model:
YouTube.rf1 <- randomForest(
formula = viewCount ~ .,
data = youtube_train)
YouTube.rf1
# Plot Model:
plot(YouTube.rf1)
# Mean Squared Error
YouTube.rf1$mse
# number of trees with lowest MSE
which.min(YouTube.rf1$mse)
# 231
# RMSE of this optimal random forest
sqrt(YouTube.rf1$mse[which.min(YouTube.rf1$mse)])
# 531857
# Validation:
# create training and validation data
set.seed(123)
valid_youtube_index <- rsample::initial_split(youtube_train, .8)
# training data
valid_youtube_train <- analysis(valid_youtube_index)
# validation data
youtube_valid <- assessment(valid_youtube_index)
# Supply the validation data in the xtest and ytest arguments:
x_test <- youtube_valid[setdiff(names(youtube_valid), "viewCount")]
y_test <- youtube_valid$viewCount
rf_oob_comp <- randomForest(
formula = viewCount ~ .,
data    = valid_youtube_train,
xtest   = x_test,
ytest   = y_test
)
# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)
# compare error rates:
tibble::tibble(
`Out of Bag Error` = oob,
`Test error` = validation,
ntrees = 1:rf_oob_comp$ntree
) %>%
gather(Metric, RMSE, -ntrees) %>%
ggplot(aes(ntrees, RMSE, color = Metric)) +
geom_line() +
scale_y_continuous(breaks=c(530000,580000, 600000, 700000)) +
xlab("Number of trees")
# Tuning
# names of features
features <- setdiff(names(youtube_train), "viewCount")
set.seed(123)
YouTube.rf2 <- tuneRF(
x          = youtube_train[features],
y          = youtube_train$viewCount,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = FALSE      # to not show real-time progress
)
# -0.02509524 0.01
# -0.005616687 0.01
plot(YouTube.rf2)
lines(YouTube.rf2, col="blue")
# Larger grid search
## hyperparameter grid search:
hyper_grid <- expand.grid(
mtry       = seq(2, 9, by = 2),
node_size  = seq(3, 9, by = 2),
sampe_size = c(.55, .632, .70, .80),
OOB_RMSE   = 0
)
for(i in 1:nrow(hyper_grid)) {
# train model
model <- ranger(
formula         = viewCount ~ .,
data            = youtube_train,
num.trees       = 500,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$node_size[i],
sample.fraction = hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
hyper_grid %>%
dplyr::arrange(OOB_RMSE) %>%
head(10)
# Best model mtry = 6, nodes = 5 and sample of 70%
# Lets repeat this model to get a better expectation of our error rate:
OOB_RMSE <- vector(mode = "numeric", length = 100)
for(i in seq_along(OOB_RMSE)) {
optimal_ranger <- ranger(
formula         = viewCount ~ .,
data            = youtube_train,
num.trees       = 500,
mtry            = 6,
min.node.size   = 5,
sample.fraction = .7,
importance      = 'impurity'
)
OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}
hist(OOB_RMSE, breaks = 20)
# Variable importance:
optimal_ranger$variable.importance %>%
broom::tidy() %>%
dplyr::arrange(desc(x)) %>%
dplyr::top_n(7) %>%
ggplot(aes(reorder(names, x), x)) +
geom_col() +
coord_flip() +theme_bw() +
ggtitle("Top 5 important variables")
rf <- randomForest(viewCount ~ ., data=youtube_train, mtry = 4)
rf
ames_ranger <- ranger(
formula   = viewCount ~ .,
data      = youtube_train,
num.trees = 500,
mtry      = 4)
ames_ranger
model_rf  <- train(viewCount ~ ., data = youtube_train, method =
"rf", importance = TRUE, mtry = 4, ntree = 500, keep.forest=TRUE)
model_rf  <- train(viewCount ~ ., data = youtube_train, method =
"rf", importance = TRUE, mtry = 4, ntree = 500)
model_rf  <- randomForest(viewCount ~ ., data = youtube_train, method =
"rf", importance = TRUE, mtry = 4, ntree = 500)
model_rf
View(model_rf$predicted)
model_rf$pred[order(model_rf$pred$rowIndex),2]
model_rf$predicted[order(model_rf$predicted$rowIndex),2]
varImpPlot(model_rf)
model_rf$test
model_rf$terms
model_rf$y
prediction_for_table <- predict(model_rf,youtube_test[,-1])
table(observed=youtube_test[,1],predicted=prediction_for_table)
data_set_size <- floor(nrow(data)/2)
indexes <- sample(1:nrow(data), size = data_set_size)
youtube_train <- data[index,]
set.seed(123)
data_set_size <- floor(nrow(data)/2)
indexes <- sample(1:nrow(data), size = data_set_size)
youtube_train <- data[index,]
youtube_split <- initial_split(data, prop = .7)
youtube_train <- training(youtube_split)
youtube_test <- testing(youtube_split)
model_rf  <- randomForest(viewCount ~ ., data = youtube_train, method =
"rf", importance = TRUE, mtry = 4, ntree = 500)
model_rf
varImpPlot(model_rf)
ames_ranger <- ranger(
formula   = viewCount ~ .,
data      = youtube_train,
num.trees = 500,
mtry      = 4)
ames_ranger
prediction_for_table <- predict(ames_ranger,youtube_test)
prediction_for_table
prediction_for_table$treetype
table(observed=youtube_test,predicted=prediction_for_table)
length(youtube_test)
length(prediction_for_table)
predict(model_rf, data=youtube_train)->Prediction
postResample(Prediction, youtube_test$viewCount)
postResample(Prediction, youtube_train$viewCount)
prediction <- predict(ames_ranger,youtube_test, type='response')
table(observed=youtube_test,predicted=prediction)
table(prediction, youtube_test$viewCount)
prediction <- predict(model_rf,youtube_test, type='response')
table(prediction, youtube_test$viewCount)
View(table(prediction, youtube_test$viewCount))
mean(table(prediction, youtube_test$viewCount))
mean(table(prediction, youtube_test))
