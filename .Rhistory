how='list')
Mod <- rapply(datlist,
lm_robust(data = datlist, formula=denote ~ migrant+ misei,
cluster=dat$idclass ),
how='list')
data1 <- list(rnorm(100),rnorm(100),rnorm(100))
View(data1)
Mod <- rapply(  datlist, FUN=function(data){
lm_robust( data=data, formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}, how = 'list'  )
Mod <- lapply(  datlist, FUN=function(data){
lm_robust( data=data, formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}, how = 'list'  )
Mod <- lapply(  datlist, FUN=function(data){
lm_robust( data=data, formula=denote ~ migrant+ misei,
cluster=dat$idclass )
})
res <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
res[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
View(res)
res <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
res[[i]]<- miceadds::lm.cluster( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
res <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
res[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
View(res)
rm(res)
res <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
res[[i]]<- miceadds::lm.cluster( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
View(res)
class(res)
res[[1]]
res_margins <- vector('list', length(res))
for(i in seq_along(res)){
for(j in seq_along(res[[i]])){
res_margins[[i]] <- margins::margins(res[[i]])
}
}
View(res)
class(res)
View(res_margins)
res_margins <- vector('list', length(res))
for(i in seq_along(res)){
res_margins[[i]] <- margins::margins(res[[i]])
}
for(i in res)){
res_margins[[i]] <- margins::margins(res[[i]])
}
for(i in res){
res_margins[[i]] <- margins::margins(res[[i]])
}
mar_cl <- margins::margins(res[[1]])
res[[1]]
dat <- fabricate(
N = 100,
x = runif(N),
clust = sample(rep(1:20, each = 5)),
y_clust = rnorm(N),
z_clust = cluster_ra(clust),
)
install.packages("estimatr")
library(estimatr)
lmout_cl <- lm_robust(
y_clust ~ z_clust + x,
data = dat,
clusters = clust
)
install.packages("estimatr")
library(estimatr)
lmout_cl <- lm_robust(
y_clust ~ z_clust + x,
data = dat,
clusters = clust
)
lmout_cl
View(res)
res <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
res[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
View(res)
mar_cl <- margins::margins(res[[1]])
res_margins <- vector('list', length(res))
for(i in seq_along(res)){
res_margins[[i]] <- margins::margins(res[[i]])
}
View(res_margins)
mar_cl
View(res)
for(i in res){
res_margins[[i]] <- margins::margins(res[[i]])
}
res_margins <- list()
for(i in seq_along(res)){
res_margins[[i]] <- margins::margins(res[[i]])
}
View(res_margins)
for(i in seq_along(res)){
res_margins[[i]] <- margins::marginal_effects(res[[i]])
}
View(res_margins)
margins <- lapply(  res, FUN=function(data){
with(data, margins::marginal_effects(data))
})
View(margins)
res_margins <- list()
for(i in seq_along(res)){
res_margins[[i]] <- margins::marginal_effects(res[[i]])
}
View(res_margins)
View(res_margins)
data(data.ma05)
dat <- data.ma05
resp <- dat[, - c(1:2) ]
imp <- mice::mice( resp, method="norm", maxit=3, m=6 )
datlist <- miceadds::mids2datlist( imp )
library(estimatr)
data(data.ma05)
dat <- data.ma05
resp <- dat[, - c(1:2) ]
imp <- mice::mice( resp, method="norm", maxit=3, m=6 )
datlist <- miceadds::mids2datlist( imp )
mod <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
r[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
mod[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
View(mod)
mod_margins <- list()
for(i in seq_along(mod)){
res_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
View(mod)
View(mod_margins)
mod_margins[[1]]
margins::margins(mod[[1]])
View(mod_margins)
mod_margins
mod
mar_cl <- margins::margins(mod[[1]])
mar_cl
mar_cl <- margins::marginal_effects(mod[[1]])
mar_cl
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginals(mod[[i]])
}
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::margins(mod[[i]])
}
View(mod_margins)
mod_margins <- list()
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
View(mod_margins)
library(mitools)
library(miceadds)
data(data.ma05)
dat <- data.ma05
summary(dat)
resp <- dat[, - c(1:2) ]
library(mitools)
library(miceadds)
data(data.ma05)
dat <- data.ma05
summary(dat)
library(estimatr)
mod <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
mod[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
resp <- dat[, - c(1:2) ]
imp <- mice::mice( resp, method="norm", maxit=3, m=6 )
datlist <- miceadds::mids2datlist( imp )
mod <- vector('list', length(datlist))
for(i in seq_along(datlist)){
for(j in seq_along(datlist[[i]])){
mod[[i]]<- lm_robust( data=datlist[[i]], formula=denote ~ migrant+ misei,
cluster=dat$idclass )
}
}
mar_mod <- lapply(Mod, FUN=function(rr){
with(rr, margins::margins(lm,vcov= vars))
})
library(margins)
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
# For single model in `mod`:
mar_cl <- margins::margins(mod[[1]])
mar_cl
summary(mod)
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
mod_margins <- list()
for(i in seq_along(mod)){
mod_margins[[i]] <- margins::marginal_effects(mod[[i]])
}
View(mod_margins)
summary(mod_margins)
mod_margins
mod
mod[[1]]$p.value
mod[[]]$p.value
mod_pvs_se <- data.frame()
mod[[1]]$std.error
for(i in seq_along(mod)){
mod_pvs_se$p-value <- mod[[i]]$p.value
mod_pvs_se$SE <- mod[[i]]$std.error
}
for(i in seq_along(mod)){
mod_pvs_se$p_value <- mod[[i]]$p.value
mod_pvs_se$SE <- mod[[i]]$std.error
}
require(data.table)
set.seed(1)
DT1 <- data.table(loc = c("L1","L2"), product = c("P1","P2","P3"), qty = runif(12))
DT2 <- data.table(product = c("P1","P2","P3"), family = c("A","A","B"), price = c(5,7,10))
require(data.table)
set.seed(1)
DT1 <- data.table(loc = c("L1","L2"), product = c("P1","P2","P3"), qty = runif(12))
DT2 <- data.table(product = c("P1","P2","P3"), family = c("A","A","B"), price = c(5,7,10))
View(DT1)
View(DT2)
library(dplyr)
library(tuber)
library(naniar)
library(ggplot2)
library(magrittr)
library(reshape2)
library(corrplot)
library(ggthemes)
library(tidyverse)
library(PerformanceAnalytics)
setwd("~/code/R/DataAccess-Extraction")
library(readxl)
tech_merged_channels <- read_excel("merged_video_stats.xlsx")
# Data Structure:
str(tech_merged_channels)
# Numerical data:
i <- c(7, 8, 9, 10, 11, 13, 14, 15, 17, 18, 20, 21, 22, 24)
tech_merged_channels[, i] <- apply(tech_merged_channels[ , i], 2,
function(x) as.numeric(as.character(x)))
# Change the date to date:
tech_merged_channels$PublishedDate <- as.Date(tech_merged_channels$PublishedDate)
# Factor:
tech_merged_channels$channel_title <- as.factor(tech_merged_channels$channel_title)
# Check for any missing values:
summary(tech_merged_channels)
# To visualise this:
gg_miss_var(tech_merged_channels)
# Trends in missing values
gg_miss_upset(tech_merged_channels)
# Heatmap
vis_miss(tech_merged_channels)
# 0.3%
# Number of columns with missing values.
n_var_miss(tech_merged_channels)
# [1] 9
# Number of rows with missing values
sum(apply(tech_merged_channels, 1, anyNA))
# [1] 77
#Count of missing values per column:
data.frame(sapply(tech_merged_channels, function(y) sum(length(which(is.na(y))))))
# Missing data by Year
gg_miss_fct(x = tech_merged_channels, fct = PublishedYear) +
labs(title = "Year NA in YouTube Data")
# Missing data by Channel:
gg_miss_fct(x = tech_merged_channels, fct = channel_title) +
labs(title = "Channel NA in YouTube Data ")
# Remove missing values.
tech_merged_channels <- tech_merged_channels[complete.cases(tech_merged_channels), ]
# You can confirm this by running the line 249
# Visualization:
# Significant Outliers in Views:
library(ggpubr)
library(rstatix)
# Data Exploration:
# View the distribution of views:
tech_merged_channels %>%
ggplot( aes(x=viewCount)) +
geom_histogram(bins = 50, fill="#0c4c8a", color="#e9ecef", alpha=0.9) +
theme_minimal() +
labs(title = "YouTube View",
subtitle = "YouTube Views from The marquesbrownlee, Dave2d, UltraLinx",
x = "View Count",
y = " ",
caption = "YouTube")
# To see outliers clearly we shall have a boxplot:
tech_merged_channels %>%
ggplot(aes(x="", y = viewCount)) +
geom_boxplot(outlier.colour="red")
theme_minimal()
# By Channel title:
tech_merged_channels %>%
ggplot(aes(x=viewCount, y = channel_title)) +
geom_boxplot(outlier.colour="red") +
theme_calc()+ scale_colour_calc() +
ggtitle("YouTube Data") +
labs(title = "YouTube View Outliers",
subtitle = "YouTube Views from The marquesbrownlee, Dave2d, UltraLinx",
x = "View Count",
y = "Channel",
caption = " Â©YouTube")
# To check outliers:
Outliers <-  tech_merged_channels %>%
group_by(channel_title) %>%
identify_outliers(viewCount)
# Significant outliers i.e. is.extreme = TRUE
Significant_Outliers <- Outliers[which(Outliers$is.extreme == TRUE),]
#Plotting the data with and without the Outliers comparing them with like counts
par(mfrow=c(1, 2))
plot(tech_merged_channels$viewCount, tech_merged_channels$likeCount,
main="With Outliers",
xlab = "View Count",
ylab = "Like Count",
pch="*",
col="red", cex=2)
abline(lm(likeCount ~ viewCount, data=tech_merged_channels), col="blue", lwd=3, lty=2)
# Without
Without_Outliers_data <- tech_merged_channels[-which(tech_merged_channels$viewCount %in% c(Outliers$viewCount)),]
plot(Without_Outliers_data$viewCount, Without_Outliers_data$likeCount,
main="Without Outliers",
xlab = "View Count",
ylab = "Like Count",
pch="*",
col="red", cex=2)
abline(lm(likeCount ~ viewCount, data=Without_Outliers_data), col="blue", lwd=3, lty=2)
data <- Without_Outliers_data[,c("viewCount","likeCount","dislikeCount",
"commentCount", "subscriberCount", "channelVideoCount",
"channelViewCount", "PrevCommentCount", "PrevDislikeCount",
"PrevLikeCount", "PrevViewCount")]
date_data <- data.frame(
# Find the number days in between posting videos:
Days = as.numeric(difftime(strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$publication_date), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"),strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$PrevPublishedAt), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"), units = "days")),
# Find the number of hours in between posting videos:
Hours_published = as.numeric(difftime(strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$publication_date), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"),strptime(format(
as.POSIXct(as.Date(Without_Outliers_data$PrevPublishedAt), format = "%m/%d/%Y %H:%M:%S"),
format="%m/%d/%Y"), format = "%m/%d/%Y"), units = "hours")),
# Age of Channel to Video publication:
Age_Channel = Without_Outliers_data$PublishedYear - Without_Outliers_data$ChannelAge)
data <- add_column(data, date_data, .after = 11)
cormat <- round(cor(data), 2)
get_upper_tri <- function(cormat){
cormat[lower.tri(cormat)]<- NA
return(cormat)
}
upper_tri <- get_upper_tri(cormat)
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+ # minimal theme
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
ggheatmap +
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
axis.title.x = element_blank(),
axis.title.y = element_blank(),
panel.grid.major = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.ticks = element_blank(),
legend.justification = c(1, 0),
legend.position = c(0.6, 0.7),
legend.direction = "horizontal")+
guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
title.position = "top", title.hjust = 0.5))
# For Correlation significance:
PerformanceAnalytics::chart.Correlation(data, pch=19)
tech_merged_channels %>%
ggplot(aes(x=PublishedDate,y=viewCount)) +
geom_line(color="steelblue", size = 0.5) +
scale_color_manual(values = c("#00AFBB", "#E7B800","#FF0000")) +
labs(title = "YouTube Videos Posted Trend from 2008 - 2021",
caption = " Source: YouTube") +
xlab("Date") + ylab("View Count") +
theme_minimal()
# There has been an increase in views with the age of the account.
# By individual accounts:
tech_merged_channels %>% ggplot(aes(x = PublishedDate , y = viewCount, color = channel_title)) +
geom_line() +
scale_color_manual(values = c("#00AFBB", "#E7B800","#FF0000")) +
labs(title = "YouTube Videos Posted Trend from 2008 - 2021",
subtitle = "Trends by Channels",
caption = " Source: YouTube") +
xlab("Date") + ylab("View Count") +
theme_minimal()
# Time Visualization:
library(lubridate)
time <- data.frame( Date = gsub("T", " ", tech_merged_channels$publication_date))
time <- data.frame(Date = as.POSIXct(gsub("Z", "", time$Date), format = "%Y-%m-%d %H:%M:%S"))
# Time of Day
time <- time %>%
add_column(Time = format(time$Date, format = "%H:%M:%S"),
# Time of Day
TOD = with(time, ifelse(Time > "053000" & Time < "120000", "Morning",
ifelse(Time >= "120000" & Time < "170000", "Afternoon",
ifelse(Time >= "170000" & Time < "200000", "Evening", "Night")))),
# Day of Week
DOW = weekdays(time$Date, abbreviate = F),
year = tech_merged_channels$PublishedYear,
channel_title = tech_merged_channels$channel_title
)
# Count by Day of the week & Time of Day:
time$DOW <- factor(time$DOW, levels= c("Sunday", "Monday",
"Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
time %>%
ggplot(aes(x=DOW, fill = TOD)) + geom_bar(position = "stack") +
coord_flip() +
scale_fill_brewer( palette = "YlGnBu" ) +
theme_minimal() + theme( legend.position = "bottom" ) +
labs(title = "YouTube Daily Trend",
subtitle = "Daily Trends by Time of Week",
caption = " Source: Â©YouTube") +
xlab("") + ylab("View Count")
# By DOW by Channel:
time %>%
ggplot(aes(x=DOW, fill = channel_title)) + geom_bar(position = "stack") +
coord_flip() +
scale_fill_brewer( palette = "YlGnBu" ) +
theme_minimal() + theme( legend.position = "bottom" ) +
labs(title = "YouTube Daily Trend",
subtitle = "Daily Trends by Channel",
caption = " Source: Â©YouTube") +
xlab("") + ylab("View Count")
install.packages(c("randomForest", "rsample", "ranger", "h20", "caret"))
install.packages("h2o")
install.packages("h2o", dependencies = TRUE)
?randomForest
library(resample)
youtube_split <- initial_split(data, prob = .7)
library(caret)
youtube_split <- createDataPartition(data, p = .7,
list = FALSE,
times = 1)
youtube_split <- createDataPartition(data$viewCount, p = .7,
list = FALSE,
times = 1)
youtube_train <- data[youtube_split,]
youtube_test <- data[youtube_split,]
View(youtube_test)
YouTube.rf1 <- randomForest(
formula = viewCount ~ .,
data = youtube_train)
library(randomForest)
YouTube.rf1 <- randomForest(
formula = viewCount ~ .,
data = youtube_train)
YouTube.rf1
install.packages("AmesHousing")
trial <- AmesHousing:make_ames()
library(AmesHousing)
trial <- AmesHousing:make_ames()
AmesHousing:make_ames()
